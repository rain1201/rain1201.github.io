{"posts":[{"title":"WR740漏洞复现","text":"1.固件获取 2.binwalk解包binwalk安装binwalk 安装 123$ git clone https://github.com/ReFirmLabs/binwalk.git$ cd binwalk$ sudo python3 setup.py install sasquatch工具支持对 非标准的SquashFS格式的文件镜像 进行解压 sasquatch 安装 1234$ sudo apt-get install build-essential liblzma-dev liblzo2-dev zlib1g-dev$ git clone https://github.com/devttys0/sasquatch.git$ cd sasquatch$ ./build.sh sasquatch patch 12345cd patcheswget https://github.com/devttys0/sasquatch/files/7776843/M1-Kali.patch.txtpatch patch0.txt M1-Kali.patch.txtcd .../build.sh binwalk解包$ binwalk -Me wr740nv1_en_3_12_4_up.bin 3.firmwalker扫描获取firmwalker：git clone https://github.com/craigz28/firmwalker.git","link":"/2024/05/07/WR740%E6%BC%8F%E6%B4%9E%E5%A4%8D%E7%8E%B0/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2024/02/09/hello-world/"},{"title":"强化学习综述","text":"摘 要 强化学习作为一种受到人类自然学习过程启发的机器学习方式，已经成为一种解决策略探索问题的主要方法。通过学习强化学习的基本概念与组成、分析相关常用算法的基本原理、展望前景与前沿挑战共三个方面概述分析强化学习，期望在完成作业的同时为对该领域一无所知的人提供有限的启发。 关键词 强化学习；策略搜索；神经网络 A Review of Reinforcement Learning Abstract Reinforcement learning, as a machine learning method inspired by the natural learning process of humans, has become a major approach for solving problems related to strategy exploration. By learning the basic concepts and components of reinforcement learning, analyzing the basic principles of commonly used algorithms, and looking forward to the prospects and frontier challenges, this article provides an overview and analysis of reinforcement learning, with the expectation of providing limited inspiration for those who know nothing about this field while completing the assignment. Key words Reinforcement learning; Policy search; Neural networks 在生物进化的过程中，物种的学习能力不断增强。而任何一个物种的学习，都离不开与自然环境的交互。他们在自然环境中进行行为的选择，根据不同行为选择获得不同的回报。再此过程中他们逐渐获得了对特定环境情况的判断能力，也就是习得了新技能。如果根据拉马克的用进废退理论，这种能力还会在亲子代间遗传，从而使整个种群不断进化。 虽然以现代科学的视角来看，拉马克的观点存在一定的局限性，但这并不妨碍其观点为当代人工智能的研发提供灵感。强化学习过程便与生物在环境中进行交互式学习有异曲同工之妙。在计算过程中，强化学习试图尽可能提高自身与环境交互而产生的收益总量。强化学习会从过往训练过程中获取信息并对自身进行修正，以此提高下一次预测的预期收益。 1 强化学习基础概念 1．1 强化学习 强化学习主要解决一个智能体如何在一个不确定的环境中通过与环境的交互而获得尽可能多的收益的问题。它通过收集环境信息，指导更好的动作。这种在交互中学习的方法被称为强化学习。 1．2 基本元素 1．2．1 环境与智能体 环境是智能体观察、感知、交互的外部系统。智能体处于环境中，能通过采取行动来改变环境。 1．2．2 状态与观察值 ​ 两者均描述环境。不同之处在于状态是对环境的完全描述，不会遗漏任何信息。而观察值可能是对环境的有限描述，可能仅包含环境的部分信息。 1．2．3 动作 ​ 不同的环境允许智能体进行不同的活动，其允许的有效动作的集合被称作动作空间，可分为离散动作空间和连续动作空间。 1．2．4 奖励 ​ 奖励是由环境给予智能体的反馈信号，是一个标量，可用于评价智能体在某一步采取动作的表现如何。 1．3 术语 1．3．1 策略 策略用于根据当前状态决定下一步执行的动作，可以是确定的或随机的，分别表示为： at=μ(st) at~π(st) 1．3．2 状态转移 用于衡量在当前环境和行动下，系统状态向某一状态转移的可能性。一般是随机的。可以使用状态密度函数表示： p(s| s, a)=P(S=s` | S=s, A=a) 1．3．3 回报 回报是对奖励的加权求和。因为未来的奖励具有不确定性，距现在越远的奖励不确定性越强。因此引入折扣率γ，减弱未来的奖励对当下回报计算的影响： Ut= Rt+γ2Rt+1+γ3Rt+2+…… 1．3．4价值函数 可分为状态价值函数和动作价值函数。前者在给定策略的情况下衡量当前当前状态的好坏；后者在给定策略和状态的情况下衡量某一动作的好坏。 2 强化学习算法 2．1 算法分类 按照环境是否已知划分，强化学习算法可分为免模型学习和有模型学习。前者不会尝试模拟环境，而后者会通过模拟环境来预判采取某个行动会发生的情况。一般来说，环境是难以模拟的，所以主要研究无模型学习。 按照学习方式划分，强化学习算法可分为在线策略和离线策略。前者必须由智能体与环境交互并学习；后者则可以观察其他个体与环境交互而进行学习。 按照学习目标划分，强化学习算法可分为基于策略和基于价值。前者直接根据概率选择动作，后者选择价值较高的动作。两者可以进行结合 2．2 Q-Learning 2．2．1 概述 Q-Learning是一种基于表格而没有神经网络参与的算法。它通过每次尝试更新Q值表，并以此指导行动。 2．2．2 算法流程 1．初始化动作价值函数Q(s,a)，其中s代表状态，a代表选择的动作。 2．依照状态s根据Q及其他策略（例如ε贪婪策略）选择动作a 3．执行动作a，观察当前状态s`与收益r 4．更新 Q(s,a)=Q(s,a)+α[r+γmaxa Q(s,a`)-Q(s,a)] s=s` 其中α为学习率, maxa Q(s,a`)代表从下一动 作中预期获得的最大收益。 5．重复2，3，4 2．2．3 ε贪婪策略 在开始时，Q(s,a)被初始化的方式可能与最优解相差很远，这可能导致学习缓慢。因此，需要权衡随机探索与利用已有经验的程度。此时，可以使用ε贪婪策略。ε表示在某一步时使用随机行动的概率。 开始时，将ε设定为1，即完全随机。这可以为Q函数的构建提供丰富的学习资源。随着训练的进行，Q函数变得更为准确，此时可以逐步缩小ε，更多地由Q函数决定采取的行动。 2．2．4 Q值表 Q的函数值是由s与a共同决定的，且在Q-Learning中，s与a都是离散的。这使得我们可以通过一张二维表格将Q函数完整表示。表格中的一维是环境的所有状态，另一维是能够采取的动作，中间则为Q值。 事实上，通过Q函数选择动作的过程就是在Q值表中选取当前状态对应的一行，找到该行中最大的Q值，并采取其列对应的动作。 2．3 Deep Q Network 2．3．1 概述 Q-Learning使用Q值表来保存Q值，这对于有较多状态的环境或连续的动作是不现实的。在DQN中，神经网络替代了Q值表，接受状态s与动作a并给出预测的Q值。 2．3．2 存在问题 1．一个极小的变化可能引起对动作选择的很大影响。 2．对于连续动作，选择最大的Q值仍比较困难。 3．无法学习到随机策略。 2．4 Policy Gradient 2．4．1 综述 在Q-learning和DQN中，计算Q值（预期收益）都是必不可少的过程，同时也是限制算法应用的一大障碍。如果能够直接根据状态选择动作，就可以避免计算Q值带来的限制，这也就是Policy Gradient。 Policy Gradient算法是一种基于梯度优化的强化学习算法。它通过优化策略来最大化预期奖励，通常用于处理连续动作空间的问题。 Policy Gradient算法使用代表状态和行动的神经网络来决定策略。该算法通过多次采样，根据每个样本的回报来计算损失函数。然后使用梯度上升来更新代理策略网络的参数，以最小化损失。 2．4. 2 算法概述 首先，初始化代理策略网络参数，然后根据当前策略生成多个样本路径。对每个样本路径计算其总收益，并将其作为回报。根据计算所得的回报，计算损失函数，并对其进行梯度计算。最后，使用梯度上升来更新代理策略网络的参数。 3 局限性 Policy Gradient算法通常需要多次采样以获得损失函数的近似值，因此计算效率相对较低。此外，Policy Gradient算法也容易受到过拟合和噪声干扰等问题的影响。为了解决这些问题，可以考虑采用正则化、调整学习率等方法来优化算法的性能。 普通的Policy Gradient算法，只适合于解决规模较小的问题，比如让杆子竖起来。如果想应用到更复杂的问题上，就需要更复杂的一些方法，比如的Actor Critic，Asynchronous Advantage Actor-Critic (A3C)。 3前景展望 强化学习，作为人工智能领域中的一个重要分支，已经展示了其在许多领域的巨大潜力和前景。无论是自动驾驶汽车、智能家居系统、还是游戏AI，强化学习算法都在解决复杂的决策问题方面表现出色。然而，强化学习也面临着一系列挑战，其中最主要的包括样本效率问题、探索与利用的平衡、以及稳定性与收敛性等。 样本效率问题是强化学习算法在大量样本数据中进行学习的问题。为了获得最佳的学习效果，算法需要大量的样本数据进行训练，这无疑增加了算法的复杂性和运行成本。探索与利用的平衡问题则是指在探索新的可能性和利用已有知识之间找到一个平衡。过于探索可能导致算法失去方向，而过于利用则可能导致算法陷入局部最优解。 稳定性与收敛性也是强化学习算法面临的挑战。在面对复杂环境和决策空间时，强化学习算法可能会出现不稳定的情况，即在不同运行中得到的结果差异较大。而收敛性则是指算法能否在有限的步骤或时间内找到最优解。 为了解决这些问题，研究人员正在不断探索新的方法和算法。例如，一些研究人员正在尝试将强化学习与其他机器学习技术相结合，以提高算法的效率和稳定性。此外，还有一些研究人员正在探索新的优化策略，以更好地平衡探索和利用，以及提高算法的收敛速度和稳定性。 尽管存在这些挑战，但强化学习在许多领域的应用前景仍然非常广阔。它将继续推动自动化系统的革命，改善我们的生活和工作方式。同时，它不仅仅局限于传统机器人领域，还能扩展至金融、医疗、游戏和教育等多个领域，以改善决策和解决优化问题。 我们期待未来的研究人员能够克服强化学习的这些挑战，提出新的算法和技术，以改进其稳定性和可解释性。随着时间的推移，强化学习有望在不同领域带来更多创新和智能化解决方案，为未来的科技发展带来更广阔的可能性。无论是在自动驾驶汽车的安全性、智能家居系统的智能化水平、还是在游戏AI的表现上，强化学习都将为我们带来更多的惊喜和突破。 致谢 感谢老师对我的指导和鼓励，让我有机会深入了解强化学习这一领域。在完成这篇综述的过程中，我不仅学习了强化学习的基本概念、算法和应用，还对相关领域的研究进展有了更深入的了解。 同时，也感谢老师对我的信任和支持，让我有机会参与一些研究项目和学术活动，进一步拓宽了我的视野和知识面。 此外，我还要感谢我的同学们和朋友们，在我们的讨论和交流中，我得到了很多启发和思路。同时，也感谢他们对我的支持和鼓励，让我有信心面对学习和生活中的挑战。 再次感谢所有帮助和支持过我的人，我会继续努力学习和研究，不断提高自己的能力和水平。 参 考 文 献 [1] 强化学习入门：基本思想和经典算法, https://zhuanlan.zhihu.com/p/466455380 [2] 强化学习入门, https://blog.csdn.net/CltCj/article/details/119445005 [3] 【强化学习】Q-Learning算法详解, https://blog.csdn.net/qq_30615903/article/details/80739243 [4] 如何理解策略梯度（Policy Gradient）算法, https://zhuanlan.zhihu.com/p/110881517 [5] 强化学习专栏 第七讲 策略梯度, https://zhuanlan.zhihu.com/p/515290741 [6]Wang Yao, Luo Junren, Zhou Yanzhong, et al. Review and Analysis of Reinforcement Learning and Evolutionary Computing Methods for Policy Exploration [J/OL]. Computer Science: 1-23 [2023-12-05] (王尧,罗俊仁,周棪忠等.面向策略探索的强化学习与进化计算方法综述分析[J/OL].计算机科学:1-23[2023-12-05].http://kns.cnki.net/kcms/detail/50.1075.TP.20230925.1333.082.html.) [7]Jiang Ying, Qi Yunsong. Overview of knowledge graph completion technology and applications for deep learning in artificial intelligence [J/OL]. Computer Measurement and Control: 1-13 [2023-12-05] (姜颖,祁云嵩.面向人工智能深度学习的知识图谱补全技术与应用综述[J/OL].计算机测量与控制:1-13[2023-12-05].http://kns.cnki.net/kcms/detail/11.4762.TP.20231017.1430.006.html.) **** Background **** Reinforcement learning has a wide range of applications, including robotics, game playing, autonomous driving, and recommendation systems. It has been used to develop agents that can play complex games like Go, at a superhuman level, navigate through complex environments, and optimize energy consumption in buildings. There are several challenges associated with reinforcement learning, including the exploration-exploitation trade-off and generalization. Researchers have developed various techniques to address these challenges, such as policy gradients, and deep reinforcement learning. They are all such powerful and creative solutions that I could never imagine. Unfortunately, this article will not improve the solutions to the questions above. Neither did the article discover any valuable problems to be explored. As is said in the abstract part, the meaning of the article is just to help the people with really poor knowledge of reinforcement learning in addition to finishing the assignment and getting the credits","link":"/2024/05/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/"},{"title":"剪刀石头布dapp开发","text":"1. 实验任务和需求开发一个剪刀石头布的智能合约，实现以下功能或特性： （1）实现剪刀石头布游戏的基本逻辑，正确判断游戏结果。 （2）将玩家提交的信息加密（暂且将取hash作为加密）储存在区块中，尽可能避免作弊。 （3）尽可能保证合约在输入非法的情况下仍能运行。 （4）使合约具有一定的可复用性。 并开发与之配套的前端程序，使合约易于使用。 2. 概要设计（实验思路）（1）使用bet函数判断用户能否加入游戏，如果可以则保存用户的地址与提交的hash （2）使用reveal函数接收用户对自己选择的披露并进行校验 （3）使用getreward函数给用户发放奖励 3. 详细设计（关键算法）（1）全局数据结构： address payable[2] player;//当前玩家地址 bytes32[2] p;//hash加密下注情况 uint[2] value;//下注金额 uint8[2] chosed;//玩家选择手势，1：剪刀 2石头 3布 uint8 numberOfPlayer;//当前玩家数量 uint8 winner;//2：未决定，3：平局，1：玩家1，0：玩家0 uint timeout;//超时时间 uint finishtime;//玩家完成提交hash时间 uint MinValue;//最小下注金额 （2）只有当玩家数量不足或上一局玩家超时过长（4倍timeout）时，bet函数才会允许新玩家加入。如果情况属于后者，则还会将下注金额分别退还给上一局两位玩家，并进行重置。同时，bet函数不允许玩家对提交内容进行修改，与现实较为一致。 （3）reveal函数对输入取hash，并与bet保存的hash比较，如果一致，则储存输入。 （4）getwinner函数判断胜者，此函数中硬编码了所有合法的情况，并会将其他情况判定为平局。 （5）getreward函数首先判断是否达到获取奖励的条件，即是否游戏双方都完成了披露、调用者是否为胜者，然后进行先更改余额再进行转账并重置合约。如果平局，则双方可分别拿回自己的下注。如果超时且仅有一方披露，则认为该方为胜者。如果超时且两方均未披露，则认为平局。 （6）reset用于重置合约状态，设置为internal以防止外部恶意调用 generatehash用于创建执行bet函数所需的hash，设置为pure，执行情况不会被记录 getinfo用于获取合约状态，主要用于前端展示，设置为view user用于获取玩家编号（0，1），非当前玩家调用会导致错误 4. 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137pragma solidity ^0.5.0;/// @title 猜拳合约/// @author sry/// @dev 仅在有限的测试中没有发生错误contract putETH { address payable[2] player;//玩家地址 bytes32[2] p;//hash加密下注情况 uint[2] value=[0,0];//下注金额 uint8[2] chosed=[0,0];//1：剪刀 2石头 3布 uint8 numberOfPlayer = 0; uint8 winner=2;//2：未决定，3：平局，1：玩家1，0：玩家0 uint timeout=30 minutes; uint finishtime=now; uint MinValue=1 wei; /// @notice 用户下注 /// @param hash 用户下注信息,传入应当为keccak256(abi.encodePacked(uint8 choose,uint secret)) /// @return hash function bet(bytes32 hash) public payable returns(bytes32){ if(now&gt;=finishtime+4*timeout &amp;&amp; msg.sender!=player[0]&amp;&amp;msg.sender!=player[1]){ uint refund; refund=value[0]; value[0]=0; player[0].transfer(refund); refund=value[1]; value[1]=0; player[1].transfer(refund); reset(); } if (numberOfPlayer&gt;=2)revert(); if (msg.value&lt;MinValue)revert(); if (numberOfPlayer==1 &amp;&amp; msg.sender==player[0])revert(); numberOfPlayer=numberOfPlayer+1; value[numberOfPlayer-1]=msg.value; p[numberOfPlayer-1]=hash; player[numberOfPlayer-1]=msg.sender; finishtime=now; return hash; } /// @notice 用户披露下注情况 /// @param choose 用户选择的手势 /// @param secret 用户自定密钥 /// @return choose function reveal(uint8 choose,uint secret) external returns(uint){ uint8 user=user(msg.sender); if (p[user]==keccak256(abi.encodePacked(choose, secret))) { chosed[user]=choose; }else revert(); return choose; } /// @notice 获取奖励，平局分别取回 /// @return reward 奖励值 function GetReward() external returns(uint){ uint reward=0; if((chosed[0]==0 || chosed[1]==0) &amp;&amp; now&lt;finishtime+timeout){ revert(); } if(now&gt;=finishtime+timeout &amp;&amp; (chosed[0]==0 || chosed[1]==0)){ uint8 user=user(msg.sender); if(chosed[0]==0 &amp;&amp; chosed[1]==0){ winner=3; } else{ if(chosed[user]==0)revert(); reward=value[0]+value[1]; value[0]=0; value[1]=0; player[user].transfer(reward); reset(); return reward;} } if(winner==2)winner=getwinner(); require(winner!=2); if(winner!=3){ if(msg.sender!=player[winner])revert(); reward=value[0]+value[1]; value[0]=0; value[1]=0; player[winner].transfer(reward); reset(); }else { uint8 user=user(msg.sender); reward=value[user]; value[user]=0; msg.sender.transfer(reward); if(value[0]==0 &amp;&amp; value[1]==0)reset(); } return reward; } /// @notice 计算胜者 /// @dev reveal()并没有检查choose的值是否合法，在此函数中，不合法的输入会被判定为平局 /// @return user 胜者编号 function getwinner() public view returns(uint8){ if(chosed[0]==chosed[1]){ return 3; } else if((chosed[0]==2 &amp;&amp; chosed[1]==1) || (chosed[0]==1 &amp;&amp; chosed[1]==3) || (chosed[0]==3 &amp;&amp; chosed[1]==2)){ return 0; } else if((chosed[1]==2 &amp;&amp; chosed[0]==1) || (chosed[1]==1 &amp;&amp; chosed[0]==3) || (chosed[1]==3 &amp;&amp; chosed[0]==2)){ return 1; } return 3; } /// @notice 重置 function reset() internal { numberOfPlayer=0; chosed[0]=0; chosed[1]=0; winner=2; finishtime=now; } /// @notice 获取用户编号 /// @dev 调用用户非当前玩家会导致revert /// @param t 用户地址 /// @return user 用户编号 function user(address t) internal view returns(uint8){ if(t==player[0]){return 0;} else if(t==player[1]){return 1;} else revert(); } /// @notice 获取当前信息 /// @return player,value,chosed function GetInfo() public view returns(address,address,uint,uint,uint8,uint8){ return (player[0],player[1],value[0],value[1],chosed[0],chosed[1]); } /// @notice 生成hash /// @dev 函数声明为pure，执行情况不会被记录 /// @param choose 选择手势 /// @param secret 密码 /// @return x 生成的hash function GenerateHash(uint8 choose,uint secret) pure public returns(bytes32){ bytes32 x; x=keccak256(abi.encodePacked(choose,secret)); return x; }} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot; /&gt; &lt;title&gt;&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;style&gt; /*--预设--*/ body { padding:0px;margin: 0px; } #lyrow, #lyrow input, #lyrow textarea { font-size:12px;font-family: 'Microsoft YaHei', '微软雅黑', MicrosoftJhengHei, '华文细黑', STHeiti, MingLiu; } #lyrow { height:100vh;width: 100vw; } #lyrow div { min-height: 18px; } #lyrow input, #lyrow textarea { border:rgb(235, 235, 235) 1px solid;border-radius: 3px;padding: 5px 8px;outline: 0; } #lyrow input:hover, #lyrow textarea:hover { border: 1px solid #6bc1f2; } /*--编辑--*/ &lt;/style&gt; &lt;div id=&quot;lyrow&quot;&gt; &lt;input type=&quot;submit&quot; name=&quot;button&quot; value=&quot;连接metamask&quot; id=&quot;connect&quot; &gt; &lt;span &gt;选择手势：&lt;/span&gt; &lt;select name=&quot;select&quot; id=&quot;choose&quot; &gt; &lt;option value=&quot;1&quot;&gt;剪刀&lt;/option&gt; &lt;option value=&quot;2&quot;&gt;石头&lt;/option&gt; &lt;option value=&quot;3&quot;&gt;布&lt;/option&gt; &lt;/select&gt; &lt;span &gt;选择账号：&lt;/span&gt; &lt;select name=&quot;select&quot; id=&quot;acc&quot; &gt; &lt;/select&gt; &lt;span &gt;下注金额(eth,支持小数)：&lt;/span&gt; &lt;input type=&quot;text&quot; name=&quot;input&quot; oninput=&quot;value=value.replace(/[^\\d.]/g,'')&quot; id=&quot;val&quot; value=&quot;1&quot;&gt; &lt;span &gt;随机数：&lt;/span&gt; &lt;input type=&quot;text&quot; name=&quot;input&quot; oninput=&quot;value=value.replace(/[^\\dabcdefx]/g,'')&quot; id=&quot;ran&quot;&gt; &lt;input type=&quot;submit&quot; name=&quot;button&quot; value=&quot;获取当前状态&quot; id=&quot;getinfo&quot; &gt; &lt;input type=&quot;submit&quot; name=&quot;button&quot; value=&quot;下注&quot; id=&quot;bet&quot; &gt; &lt;input type=&quot;submit&quot; name=&quot;button&quot; value=&quot;披露下注情况&quot; id=&quot;reveal&quot; &gt; &lt;input type=&quot;submit&quot; name=&quot;button&quot; value=&quot;查看结果&quot; id=&quot;getwinner&quot; &gt; &lt;input type=&quot;submit&quot; name=&quot;button&quot; value=&quot;获取奖励&quot; id=&quot;getreward&quot; &gt; &lt;/div&gt; &lt;div id=&quot;player-content&quot;&gt; &lt;/div&gt; &lt;script src=&quot;js/web3.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;js/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;js/index.js&quot;&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258$(function() { var account = null; var web3 = null; const address = '0xb92b82a94382b9110e8febBEd4AEfB7F3436a26F';//'0xa7228a391b6B0E343C33F6aE5A511Eefaf540688';//'0xd6C500DccEF6B65eF956B7446a405cbed4c47814' ;// 合约部署地址 var contract =null; var choose=0; var r=null; const abi =[ { &quot;constant&quot;: true, &quot;inputs&quot;: [ { &quot;internalType&quot;: &quot;uint8&quot;, &quot;name&quot;: &quot;choose&quot;, &quot;type&quot;: &quot;uint8&quot; }, { &quot;internalType&quot;: &quot;uint256&quot;, &quot;name&quot;: &quot;secret&quot;, &quot;type&quot;: &quot;uint256&quot; } ], &quot;name&quot;: &quot;GenerateHash&quot;, &quot;outputs&quot;: [ { &quot;internalType&quot;: &quot;bytes32&quot;, &quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;bytes32&quot; } ], &quot;payable&quot;: false, &quot;stateMutability&quot;: &quot;view&quot;, &quot;type&quot;: &quot;function&quot; }, { &quot;constant&quot;: true, &quot;inputs&quot;: [], &quot;name&quot;: &quot;GetInfo&quot;, &quot;outputs&quot;: [ { &quot;internalType&quot;: &quot;address&quot;, &quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;address&quot; }, { &quot;internalType&quot;: &quot;address&quot;, &quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;address&quot; }, { &quot;internalType&quot;: &quot;uint256&quot;, &quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;uint256&quot; }, { &quot;internalType&quot;: &quot;uint256&quot;, &quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;uint256&quot; }, { &quot;internalType&quot;: &quot;uint8&quot;, &quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;uint8&quot; }, { &quot;internalType&quot;: &quot;uint8&quot;, &quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;uint8&quot; } ], &quot;payable&quot;: false, &quot;stateMutability&quot;: &quot;view&quot;, &quot;type&quot;: &quot;function&quot; }, { &quot;constant&quot;: false, &quot;inputs&quot;: [], &quot;name&quot;: &quot;GetReward&quot;, &quot;outputs&quot;: [ { &quot;internalType&quot;: &quot;uint256&quot;, &quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;uint256&quot; } ], &quot;payable&quot;: false, &quot;stateMutability&quot;: &quot;nonpayable&quot;, &quot;type&quot;: &quot;function&quot; }, { &quot;constant&quot;: false, &quot;inputs&quot;: [ { &quot;internalType&quot;: &quot;bytes32&quot;, &quot;name&quot;: &quot;hash&quot;, &quot;type&quot;: &quot;bytes32&quot; } ], &quot;name&quot;: &quot;bet&quot;, &quot;outputs&quot;: [ { &quot;internalType&quot;: &quot;bytes32&quot;, &quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;bytes32&quot; } ], &quot;payable&quot;: true, &quot;stateMutability&quot;: &quot;payable&quot;, &quot;type&quot;: &quot;function&quot; }, { &quot;constant&quot;: true, &quot;inputs&quot;: [], &quot;name&quot;: &quot;getwinner&quot;, &quot;outputs&quot;: [ { &quot;internalType&quot;: &quot;uint8&quot;, &quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;uint8&quot; } ], &quot;payable&quot;: false, &quot;stateMutability&quot;: &quot;view&quot;, &quot;type&quot;: &quot;function&quot; }, { &quot;constant&quot;: false, &quot;inputs&quot;: [ { &quot;internalType&quot;: &quot;uint8&quot;, &quot;name&quot;: &quot;choose&quot;, &quot;type&quot;: &quot;uint8&quot; }, { &quot;internalType&quot;: &quot;uint256&quot;, &quot;name&quot;: &quot;secret&quot;, &quot;type&quot;: &quot;uint256&quot; } ], &quot;name&quot;: &quot;reveal&quot;, &quot;outputs&quot;: [ { &quot;internalType&quot;: &quot;uint256&quot;, &quot;name&quot;: &quot;&quot;, &quot;type&quot;: &quot;uint256&quot; } ], &quot;payable&quot;: false, &quot;stateMutability&quot;: &quot;nonpayable&quot;, &quot;type&quot;: &quot;function&quot; }]; function handleAccountsChanged(accounts) { var acc=document.getElementById(&quot;acc&quot;); if (accounts.length === 0) { console.log('Please connect to MetaMask.'); } else{ acc.innerHTML=&quot;&quot;; for (var a=0; a&lt;accounts.length;a++){ acc.innerHTML+=`&lt;option value=&quot;${accounts[a]}&quot;&gt;${accounts[a]}&lt;/option&gt;\\n`; }; account = acc.value; console.log(account); } } function changeaccount(){ var acc=document.getElementById(&quot;acc&quot;); account = acc.value; console.log(account); } function connect(){ ethereum .request({ method: 'eth_requestAccounts' }) .then(handleAccountsChanged) .catch((err) =&gt; { if (err.code === 4001) { console.log('Please connect to MetaMask.'); } else { console.error(err); } }); web3 = new Web3(window.web3.currentProvider); contract= new web3.eth.Contract(abi, address); } function bet(){ var sel=document.getElementById(&quot;choose&quot;); var val=document.getElementById(&quot;val&quot;); var ran=document.getElementById(&quot;ran&quot;); var hash; web3.utils.randomHex(32); contract.methods.GenerateHash(sel.value,parseInt(ran.value)).call().then(function(result) { hash=result;//web3.utils.hexToAscii(result); console.log(hash); contract.methods.bet(hash).send({ from: account,gasPrice: &quot;1000000000&quot;,value: String(parseFloat(val.value)*1000000000000000000) }) .on('error', function(error, receipt) {alert(error,receipt);return;}) .on('receipt', (data) =&gt; { console.log(data); }); }); sel.disabled=true; val.disabled=true; ran.disabled=true; } function reveal(){ var sel=document.getElementById(&quot;choose&quot;); var val=document.getElementById(&quot;val&quot;); var ran=document.getElementById(&quot;ran&quot;); contract.methods.reveal(sel.value,parseInt(ran.value)).send({from: account,gasPrice: &quot;1000000000&quot;}) .on('error', function(error, receipt) {alert(error,receipt);return;}) .then(function(result) { console.log(result); }); sel.disabled=false; val.disabled=false; ran.disabled=false; } function getreward(){ var sel=document.getElementById(&quot;choose&quot;); contract.methods.GetReward().send({from: account,gasPrice: &quot;1000000000&quot;}) .on('error', function(error, receipt) {alert(error,receipt);return;}) .then(function(result) { console.log(result); }); } function getinf(){ var res=[&quot;have not revealed&quot;,&quot;剪刀&quot;,&quot;石头&quot;,&quot;布&quot;]; contract.methods.GetInfo().call().then(function(result) { console.log(result); alert(&quot;player 0:&quot;+result[0]+&quot;\\n&quot;+ &quot;player 1:&quot;+result[1]+&quot;\\n&quot;+ &quot;value 0:&quot;+result[2]+&quot;\\n&quot;+ &quot;value 1:&quot;+result[3]+&quot;\\n&quot;+ &quot;choose 0:&quot;+res[parseInt(result[4])]+&quot;\\n&quot;+ &quot;choose 1:&quot;+res[parseInt(result[5])]+&quot;\\n&quot; ); }); } function getwinner(){ var res=[&quot;player 0 wins&quot;,&quot;player 1 wins&quot;,&quot;have not decided&quot;,&quot;draw/other situations&quot;]; contract.methods.getwinner().call().then(function(result) { alert(res[parseInt(result)]); }); } $(&quot;#connect&quot;).on('click', connect); $(&quot;#getinfo&quot;).on('click', getinf); $(&quot;#bet&quot;).on('click', bet); $(&quot;#reveal&quot;).on('click', reveal); $(&quot;#getwinner&quot;).on('click', getwinner); $(&quot;#getreward&quot;).on('click', getreward); $(&quot;#acc&quot;).on('change', changeaccount); document.getElementById(&quot;ran&quot;).value=Web3.utils.randomHex(32); }) 5. 测试方案创建两个账户模拟两个玩家进行游戏，注意完全模拟输赢和平局情况，并模拟一些非法输入或超时的情况。创建第三个账户，模拟非玩家调用情况。将合约部署至测试链，将前端部署至服务器，测试前端工作情况。 6. 测试过程先在remix vm中进行测试。 （secret的类型为uint256，实际前端会随机生成32byte作为secret，足够安全。为了测试方便使用较简单的secret） 生成hash： ​ 下注及执行后情况： ​ 披露： 对另一账号进行相似操作，得结果： （超时处理的效果难以展示，从略） 7. 结果分析在有限的测试中，合约和前端都正常工作。合约能够正确执行游戏逻辑，并具有一定的安全性和抵抗非法输入的能力。前端能够与合约恰当配合，具有一定的易用性。 8. 总结本实验设计了一个剪刀石头布的智能合约，并配套开发了前端页面，且进行了相关测试，基本完成了实验的目标与需求。 但是受限于开发时间、开发条件等因素，本实验也存在许多可完善之处。例如合约中超时处理的逻辑可以进一步优化，合约整体可以进一步优化以节省gas，前端较为简陋，前端与合约的交互可以通过event而不一定是函数返回值。这些问题有待于进一步解决完善。 在此实验中，Microsoft Azure提供了托管前端的服务器资源、Cloudflare提供了域名解析与CDN服务、ethereum.org提供了智能合约的集成开发与测试环境、智谷提供了合约最终部署环境，在此一并向它们表示感谢。","link":"/2024/05/09/%E5%89%AA%E5%88%80%E7%9F%B3%E5%A4%B4%E5%B8%83dapp%E5%BC%80%E5%8F%91/"}],"tags":[],"categories":[],"pages":[{"title":"about","text":"About","link":"/about/index.html"}]}